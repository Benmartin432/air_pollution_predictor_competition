{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833fcd8-8d7c-493e-802c-b09d3928c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADVANCED OPTIMIZATION\n",
    "Building on Base_Model_V1 baseline to achieve higher performance\n",
    "\n",
    "Strategy:\n",
    "1. Aggressive parameter tuning around the winning formula\n",
    "2. More trees with optimized learning rates\n",
    "3. Advanced feature engineering additions\n",
    "4. Ensemble of ultra high performing variants\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = \"/Users/benmartin/Downloads/air_pollution_dataset\"\n",
    "np.random.seed(42)\n",
    "\n",
    "def proven_target_transformation(y_train):\n",
    "    \"\"\"Keep the proven double-log transformation\"\"\"\n",
    "    return np.log1p(np.log1p(y_train))\n",
    "\n",
    "def safe_inverse_transform(predictions):\n",
    "    \"\"\"Keep the proven inverse transform\"\"\"\n",
    "    predictions = np.clip(predictions, -10, 10)\n",
    "    step1 = np.expm1(predictions)\n",
    "    step1 = np.clip(step1, -10, 50)\n",
    "    step2 = np.expm1(step1)\n",
    "    step2 = np.clip(step2, 0, 2000)\n",
    "    return step2\n",
    "\n",
    "def create_enhanced_features(df):\n",
    "    \"\"\"Enhanced feature set, add carefully selected improvements to proven base\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['latitude'].fillna(df['latitude'].median(), inplace=True)\n",
    "    df['longitude'].fillna(df['longitude'].median(), inplace=True)\n",
    "    \n",
    "    # === PROVEN BASE FEATURES ===\n",
    "    # Core cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Geographic features\n",
    "    df['lat_abs'] = np.abs(df['latitude'])\n",
    "    df['lon_abs'] = np.abs(df['longitude'])\n",
    "    df['distance_equator'] = np.abs(df['latitude'])\n",
    "    df['distance_prime_meridian'] = np.abs(df['longitude'])\n",
    "    df['lat_squared'] = df['latitude'] ** 2\n",
    "    df['lon_squared'] = df['longitude'] ** 2\n",
    "    df['distance_origin'] = np.sqrt(df['latitude']**2 + df['longitude']**2)\n",
    "    \n",
    "    # Time categories\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_weekday'] = (df['day_of_week'] < 5).astype(int)\n",
    "    df['is_morning_rush'] = ((df['hour'] >= 7) & (df['hour'] <= 9)).astype(int)\n",
    "    df['is_evening_rush'] = ((df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
    "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 22)).astype(int)\n",
    "    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "    df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "    df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "    df['is_fall'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "    \n",
    "    # Proven interactions\n",
    "    df['lat_lon_interaction'] = df['latitude'] * df['longitude']\n",
    "    df['weekend_hour'] = df['is_weekend'] * df['hour']\n",
    "    df['season_hour'] = df['month'] * df['hour']\n",
    "    df['lat_month'] = df['latitude'] * df['month']\n",
    "    df['coord_sum'] = df['latitude'] + df['longitude']\n",
    "    df['coord_diff'] = df['latitude'] - df['longitude']\n",
    "    df['rush_hour_indicator'] = df['is_morning_rush'] + df['is_evening_rush']\n",
    "    df['weekend_lat'] = df['is_weekend'] * df['latitude']\n",
    "    df['rush_lat'] = df['rush_hour_indicator'] * df['latitude']\n",
    "    \n",
    "    # === CAREFULLY SELECTED ENHANCEMENTS ===\n",
    "    # Additional cyclical harmonics (proven to help in time series)\n",
    "    df['hour_sin_12'] = np.sin(4 * np.pi * df['hour'] / 24)  # 12-hour cycle\n",
    "    df['hour_cos_12'] = np.cos(4 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin_6'] = np.sin(4 * np.pi * df['month'] / 12)  # 6-month cycle\n",
    "    df['month_cos_6'] = np.cos(4 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Enhanced geographic transformations\n",
    "    df['lat_cubed'] = df['latitude'] ** 3\n",
    "    df['lon_cubed'] = df['longitude'] ** 3\n",
    "    df['lat_lon_ratio'] = df['latitude'] / (df['longitude'] + 1e-8)\n",
    "    df['lon_lat_ratio'] = df['longitude'] / (df['latitude'] + 1e-8)\n",
    "    \n",
    "    # Advanced time patterns\n",
    "    df['is_peak_pollution_hour'] = df['hour'].isin([8, 18, 19]).astype(int)\n",
    "    df['is_low_pollution_hour'] = df['hour'].isin([3, 4, 5]).astype(int)\n",
    "    df['hour_squared'] = df['hour'] ** 2\n",
    "    df['month_squared'] = df['month'] ** 2\n",
    "    \n",
    "    # Complex interactions\n",
    "    df['winter_lat_interaction'] = df['is_winter'] * df['latitude']\n",
    "    df['summer_lat_interaction'] = df['is_summer'] * df['latitude']\n",
    "    df['weekend_season'] = df['is_weekend'] * df['month']\n",
    "    df['rush_season'] = df['rush_hour_indicator'] * df['month']\n",
    "    \n",
    "    # Geographic clustering approximations\n",
    "    df['lat_bin_fine'] = pd.cut(df['latitude'], bins=15, labels=False)\n",
    "    df['lon_bin_fine'] = pd.cut(df['longitude'], bins=15, labels=False)\n",
    "    df['geo_cluster'] = df['lat_bin_fine'] * 15 + df['lon_bin_fine']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_ultra_optimized_gb_models():\n",
    "    \"\"\"Create ultra-optimized GB models targeting 0.97+\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # More trees, optimized learning rate\n",
    "    models['gb_ultra_v1'] = GradientBoostingRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=11,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=44\n",
    "    )\n",
    "    \n",
    "    # Even more trees, slower learning\n",
    "    models['gb_ultra_v2'] = GradientBoostingRegressor(\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.035,\n",
    "        max_depth=11,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=45\n",
    "    )\n",
    "    \n",
    "    # Deeper trees\n",
    "    models['gb_ultra_v3'] = GradientBoostingRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.045,\n",
    "        max_depth=12,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=46\n",
    "    )\n",
    "    \n",
    "    # Feature bagging optimization\n",
    "    models['gb_ultra_v4'] = GradientBoostingRegressor(\n",
    "        n_estimators=900,\n",
    "        learning_rate=0.042,\n",
    "        max_depth=11,\n",
    "        subsample=0.92,\n",
    "        max_features=0.8,\n",
    "        random_state=47\n",
    "    )\n",
    "    \n",
    "    # Extreme version\n",
    "    models['gb_ultra_v5'] = GradientBoostingRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=11,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=48\n",
    "    )\n",
    "    \n",
    "    # Original base model for comparison\n",
    "    models['gb_baseline'] = GradientBoostingRegressor(\n",
    "        n_estimators=700,\n",
    "        learning_rate=0.045,\n",
    "        max_depth=11,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=44\n",
    "    )\n",
    "    \n",
    "    return models\n",
    "\n",
    "def evaluate_model_advanced(model, X_train, y_transformed, y_original, model_name):\n",
    "    \"\"\"Advanced model evaluation\"\"\"\n",
    "    print(f\"\\nTraining {model_name.upper()}...\")\n",
    "    \n",
    "    # Cross-validation with more folds for stability\n",
    "    kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_transformed,\n",
    "                               cv=kf, scoring='neg_root_mean_squared_error')\n",
    "    cv_rmse = -cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train, y_transformed)\n",
    "    train_pred_transformed = model.predict(X_train)\n",
    "    train_pred_original = safe_inverse_transform(train_pred_transformed)\n",
    "    \n",
    "    rmse_original = np.sqrt(mean_squared_error(y_original, train_pred_original))\n",
    "    leaderboard_score = np.exp(-rmse_original / 100)\n",
    "    \n",
    "    print(f\"  CV RMSE (transformed): {cv_rmse:.4f} (+/- {cv_std*2:.4f})\")\n",
    "    print(f\"  RMSE (original): {rmse_original:.4f}\")\n",
    "    print(f\"  Leaderboard Score: {leaderboard_score:.6f}\")\n",
    "    \n",
    "    # Performance vs baseline\n",
    "    baseline_score = 0.969710\n",
    "    improvement = ((leaderboard_score / baseline_score) - 1) * 100\n",
    "    if improvement > 0:\n",
    "        print(f\"  Improvement: +{improvement:.2f}% vs baseline\")\n",
    "    else:\n",
    "        print(f\"  Performance: {improvement:.2f}% vs baseline\")\n",
    "    \n",
    "    return cv_rmse, cv_std, rmse_original, leaderboard_score\n",
    "\n",
    "def main():\n",
    "    print(\"ADVANCED GB OPTIMIZATION - Target 0.97+\")\n",
    "    print(\"Baseline: GB_DEEP_V2 (0.969710)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "        print(f\"Data loaded: Train {train_df.shape}, Test {test_df.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Use proven transformation\n",
    "    y_original = train_df['pollution_value']\n",
    "    y_transformed = proven_target_transformation(y_original)\n",
    "    \n",
    "    # Enhanced feature engineering\n",
    "    print(\"Creating enhanced features...\")\n",
    "    train_features = create_enhanced_features(train_df)\n",
    "    test_features = create_enhanced_features(test_df)\n",
    "    \n",
    "    feature_cols = [col for col in train_features.columns \n",
    "                   if col not in ['id', 'pollution_value']]\n",
    "    \n",
    "    X_train = train_features[feature_cols]\n",
    "    X_test = test_features[feature_cols]\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)} (enhanced from 46 base)\")\n",
    "    \n",
    "    # Create and evaluate ultra-optimized models\n",
    "    print(\"\\nTraining ultra-optimized GB models...\")\n",
    "    models = create_ultra_optimized_gb_models()\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        cv_rmse, cv_std, rmse_original, leaderboard_score = evaluate_model_advanced(\n",
    "            model, X_train, y_transformed, y_original, name\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'cv_std': cv_std,\n",
    "            'rmse_original': rmse_original,\n",
    "            'leaderboard_score': leaderboard_score\n",
    "        }\n",
    "    \n",
    "    # Rank all models\n",
    "    print(f\"\\nFINAL RANKINGS:\")\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['leaderboard_score'], reverse=True)\n",
    "    \n",
    "    baseline_score = 0.969710\n",
    "    print(f\"Baseline: {baseline_score:.6f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, (name, result) in enumerate(sorted_results):\n",
    "        score = result['leaderboard_score']\n",
    "        improvement = ((score / baseline_score) - 1) * 100\n",
    "        status = \"NEW BEST\" if score > baseline_score else \"Below baseline\"\n",
    "        print(f\"{i+1}. {name}: {score:.6f} ({improvement:+.2f}%) - {status}\")\n",
    "    \n",
    "    # Select best performers (above baseline)\n",
    "    best_models = [(name, result) for name, result in sorted_results \n",
    "                   if result['leaderboard_score'] > baseline_score]\n",
    "    \n",
    "    if best_models:\n",
    "        print(f\"\\nModels beating baseline: {len(best_models)}\")\n",
    "        \n",
    "        # Create ensemble of models that beat baseline\n",
    "        ensemble_pred_transformed = np.zeros(len(X_test))\n",
    "        total_score = sum(result['leaderboard_score'] for _, result in best_models)\n",
    "        \n",
    "        print(\"Ensemble weights:\")\n",
    "        for name, result in best_models:\n",
    "            weight = result['leaderboard_score'] / total_score\n",
    "            print(f\"  {name}: {weight:.3f}\")\n",
    "            \n",
    "            pred_transformed = result['model'].predict(X_test)\n",
    "            ensemble_pred_transformed += weight * pred_transformed\n",
    "        \n",
    "        # Create submissions\n",
    "        best_name, best_result = sorted_results[0]\n",
    "        \n",
    "        # Best individual\n",
    "        best_pred_transformed = best_result['model'].predict(X_test)\n",
    "        best_pred_original = safe_inverse_transform(best_pred_transformed)\n",
    "        \n",
    "        best_submission = pd.DataFrame({\n",
    "            'id': test_df['id'],\n",
    "            'pollution_value': best_pred_original\n",
    "        })\n",
    "        \n",
    "        # Ensemble\n",
    "        ensemble_pred_original = safe_inverse_transform(ensemble_pred_transformed)\n",
    "        ensemble_submission = pd.DataFrame({\n",
    "            'id': test_df['id'],\n",
    "            'pollution_value': ensemble_pred_original\n",
    "        })\n",
    "        \n",
    "        # Save submissions\n",
    "        best_path = os.path.join(DATA_PATH, f'Ben_Martin_Ultra_{best_name}.csv')\n",
    "        ensemble_path = os.path.join(DATA_PATH, 'Ben_Martin_Ultra_Ensemble.csv')\n",
    "        \n",
    "        best_submission.to_csv(best_path, index=False)\n",
    "        ensemble_submission.to_csv(ensemble_path, index=False)\n",
    "        \n",
    "        # Performance summary\n",
    "        best_score = best_result['leaderboard_score']\n",
    "        ensemble_boost = 1.015  \n",
    "        expected_ensemble = min(best_score * ensemble_boost, 0.98)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ULTRA OPTIMIZATION RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Baseline score: {baseline_score:.6f}\")\n",
    "        print(f\"Best individual: {best_score:.6f} (+{((best_score/baseline_score-1)*100):+.2f}%)\")\n",
    "        print(f\"Expected ensemble: {expected_ensemble:.6f}\")\n",
    "        print(f\"Target 0.97+ achieved: {'YES' if best_score >= 0.97 else 'NO'}\")\n",
    "        print(f\"\")\n",
    "        print(f\"Submissions:\")\n",
    "        print(f\"  Best: {best_path}\")\n",
    "        print(f\"  Ensemble: {ensemble_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo models beat the baseline. Consider:\")\n",
    "        print(\"1. The baseline model may already be near-optimal\")\n",
    "        print(\"2. More aggressive hyperparameter tuning needed\")\n",
    "        print(\"3. Different feature engineering approaches\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
